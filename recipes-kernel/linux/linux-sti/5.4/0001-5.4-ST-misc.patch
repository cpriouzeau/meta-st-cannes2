From 217368fb1ab099595d060a5dc91560ced4427465 Mon Sep 17 00:00:00 2001
From: Christophe Priouzeau <christophe.priouzeau@st.com>
Date: Fri, 13 Dec 2019 15:26:03 +0100
Subject: [PATCH 1/2] 5.4-ST-misc

---
 arch/arm/include/asm/outercache.h |  10 +++
 arch/arm/include/asm/tz.h         |  95 +++++++++++++++++++++++++++
 arch/arm/mach-sti/board-dt.c      |   9 ---
 arch/arm/mm/cache-l2x0.c          | 103 ++++++++++++++++++++++--------
 drivers/clocksource/Kconfig       |   2 +-
 drivers/i2c/busses/i2c-st.c       |  31 +++++++--
 6 files changed, 209 insertions(+), 41 deletions(-)
 create mode 100644 arch/arm/include/asm/tz.h

diff --git a/arch/arm/include/asm/outercache.h b/arch/arm/include/asm/outercache.h
index 336463775..a7837e1e6 100644
--- a/arch/arm/include/asm/outercache.h
+++ b/arch/arm/include/asm/outercache.h
@@ -27,6 +27,8 @@ struct outer_cache_fns {
 	/* This is an ARM L2C thing */
 	void (*write_sec)(unsigned long, unsigned);
 	void (*configure)(const struct l2x0_regs *);
+
+	bool (*tz_mutex)(unsigned long *);
 };
 
 extern struct outer_cache_fns outer_cache;
@@ -117,4 +119,12 @@ static inline void outer_resume(void) { }
 
 #endif
 
+#define outer_tz_mutex outer_tz_mutex
+static inline bool outer_tz_mutex(unsigned long *lock)
+{
+	if (outer_cache.tz_mutex)
+		return outer_cache.tz_mutex(lock);
+	return false;
+}
+
 #endif	/* __ASM_OUTERCACHE_H */
diff --git a/arch/arm/include/asm/tz.h b/arch/arm/include/asm/tz.h
new file mode 100644
index 000000000..f1ff6c275
--- /dev/null
+++ b/arch/arm/include/asm/tz.h
@@ -0,0 +1,95 @@
+#ifndef __ASM_ARM_TZ_H
+#define __ASM_ARM_TZ_H
+
+/* Support of ARMv7 TrustZone generic features */
+#if (__LINUX_ARM_ARCH__ < 7) || !defined(CONFIG_SMP)
+
+/* no ATM TZ or no SMP => no need for shared spinlocks */
+static inline void tz_spin_lock(unsigned long *lock) { }
+static inline int tz_spin_trylock(unsigned long *lock) { return 1; }
+static inline void tz_spin_unlock(unsigned long *lock) { }
+
+#else
+
+#include <asm/processor.h>
+
+/*
+ * Shared spinning mutex support
+ *
+ * Shared mutex between linux and TrustZone worlds require use of very basic
+ * ARMv6+ DDR mutex cells:
+ * - lock is defined by the value stored in a 32bit DDR cell (4 byte aligned).
+ * - value is 0: mutex is not locked, value is 1: mutex is locked.
+ * - use of ldrex/strex instructions and a memory barrier when required.
+ * - basic power saving: WFE while lock is locked, SEV on lock release.
+ * - no extra complexity.
+ *
+ * Actually, this is the pre kernel 3.6 ARM arch_spinlock support.
+ */
+static inline void tz_spin_lock(unsigned long *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+"1:	ldrex	%0, [%1]\n"
+"	teq	%0, #0\n"
+	WFE("ne")
+"	strexeq	%0, %2, [%1]\n"
+"	teqeq	%0, #0\n"
+"	bne	1b"
+	: "=&r" (tmp)
+	: "r" (lock), "r" (1)
+	: "cc");
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+}
+
+static inline int tz_spin_trylock(unsigned long *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+"	ldrex	%0, [%1]\n"
+"	teq	%0, #0\n"
+"	strexeq	%0, %2, [%1]"
+	: "=&r" (tmp)
+	: "r" (lock), "r" (1)
+	: "cc");
+
+	if (tmp)
+		return 0;
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+	return 1;
+}
+
+static inline void tz_spin_unlock(unsigned long *lock)
+{
+/*
+ * ARMv6+ ticket-based spin-locking.
+ *
+ * A memory barrier is required after we get a lock, and before we
+ * release it, because V6+ CPUs are assumed to have weakly ordered
+ * memory.
+ */
+	smp_mb();
+	*lock = 0;
+
+	dsb_sev();
+}
+#endif
+
+#endif /* __ASM_ARM_TZ_H */
+
diff --git a/arch/arm/mach-sti/board-dt.c b/arch/arm/mach-sti/board-dt.c
index dcb98937f..ffecbf296 100644
--- a/arch/arm/mach-sti/board-dt.c
+++ b/arch/arm/mach-sti/board-dt.c
@@ -20,14 +20,6 @@ static const char *const stih41x_dt_match[] __initconst = {
 	NULL
 };
 
-static void sti_l2_write_sec(unsigned long val, unsigned reg)
-{
-	/*
-	 * We can't write to secure registers as we are in non-secure
-	 * mode, until we have some SMI service available.
-	 */
-}
-
 DT_MACHINE_START(STM, "STi SoC with Flattened Device Tree")
 	.dt_compat	= stih41x_dt_match,
 	.l2c_aux_val	= L2C_AUX_CTRL_SHARED_OVERRIDE |
@@ -36,5 +28,4 @@ DT_MACHINE_START(STM, "STi SoC with Flattened Device Tree")
 			  L2C_AUX_CTRL_WAY_SIZE(4),
 	.l2c_aux_mask	= 0xc0000fff,
 	.smp		= smp_ops(sti_smp_ops),
-	.l2c_write_sec	= sti_l2_write_sec,
 MACHINE_END
diff --git a/arch/arm/mm/cache-l2x0.c b/arch/arm/mm/cache-l2x0.c
index 12c26eb88..4cbc3d5f9 100644
--- a/arch/arm/mm/cache-l2x0.c
+++ b/arch/arm/mm/cache-l2x0.c
@@ -20,6 +20,7 @@
 #include <asm/hardware/cache-l2x0.h>
 #include <asm/hardware/cache-aurora-l2.h>
 #include "cache-tauros3.h"
+#include <asm/tz.h>
 
 struct l2c_init_data {
 	const char *type;
@@ -72,6 +73,26 @@ static void l2c_write_sec(unsigned long val, void __iomem *base, unsigned reg)
 		writel_relaxed(val, base + reg);
 }
 
+/*
+ * Shared mutex to synchronise L2CC maintenance between linux
+ * world and secure world (ARM TZ).
+ */
+static unsigned long *l2x0_tz_mutex;
+
+#define l2x0_spin_lock_irqsave(_flags) \
+	do {							\
+		raw_spin_lock_irqsave(&l2x0_lock, _flags);	\
+		if (l2x0_tz_mutex != NULL)				\
+			tz_spin_lock(l2x0_tz_mutex);		\
+	} while (0)
+
+#define l2x0_spin_unlock_irqrestore(_flags) \
+	do {							\
+		if (l2x0_tz_mutex != NULL)				\
+			tz_spin_unlock(l2x0_tz_mutex);		\
+		raw_spin_unlock_irqrestore(&l2x0_lock, _flags);	\
+	} while (0)
+
 /*
  * This should only be called when we have a requirement that the
  * register be written due to a work-around, as platforms running
@@ -236,6 +257,28 @@ static void l2c210_sync(void)
 	__l2c210_cache_sync(l2x0_base);
 }
 
+/* Enable/disable external mutex shared with ARM TZ */
+static bool l2x0_tz_mutex_cfg(unsigned long *lock)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&l2x0_lock, flags);
+
+	if (lock && l2x0_tz_mutex && (lock != l2x0_tz_mutex)) {
+		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		pr_err("%s: a TZ mutex is already enabled\n", __func__);
+		return false;
+	}
+
+	l2x0_tz_mutex = lock;
+	/* Ensure mutex pointer is updated before lock is released */
+	smp_wmb();
+
+	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	pr_info("%s: %sable TZ mutex\n\n", __func__, (lock) ? "en" : "dis");
+	return true;
+}
+
 static const struct l2c_init_data l2c210_data __initconst = {
 	.type = "L2C-210",
 	.way_size_0 = SZ_8K,
@@ -252,6 +295,7 @@ static const struct l2c_init_data l2c210_data __initconst = {
 		.disable = l2c_disable,
 		.sync = l2c210_sync,
 		.resume = l2c_resume,
+		.tz_mutex = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -275,10 +319,10 @@ static void l2c220_op_way(void __iomem *base, unsigned reg)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + reg);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static unsigned long l2c220_op_pa_range(void __iomem *reg, unsigned long start,
@@ -296,8 +340,8 @@ static unsigned long l2c220_op_pa_range(void __iomem *reg, unsigned long start,
 		}
 
 		if (blk_end < end) {
-			raw_spin_unlock_irqrestore(lock, flags);
-			raw_spin_lock_irqsave(lock, flags);
+			l2x0_spin_unlock_irqrestore(flags);
+			l2x0_spin_lock_irqsave(flags);
 		}
 	}
 
@@ -309,7 +353,7 @@ static void l2c220_inv_range(unsigned long start, unsigned long end)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	if ((start | end) & (CACHE_LINE_SIZE - 1)) {
 		if (start & (CACHE_LINE_SIZE - 1)) {
 			start &= ~(CACHE_LINE_SIZE - 1);
@@ -328,7 +372,7 @@ static void l2c220_inv_range(unsigned long start, unsigned long end)
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_clean_range(unsigned long start, unsigned long end)
@@ -342,12 +386,12 @@ static void l2c220_clean_range(unsigned long start, unsigned long end)
 		return;
 	}
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	flags = l2c220_op_pa_range(base + L2X0_CLEAN_LINE_PA,
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_CLEAN_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_flush_range(unsigned long start, unsigned long end)
@@ -361,12 +405,12 @@ static void l2c220_flush_range(unsigned long start, unsigned long end)
 		return;
 	}
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	flags = l2c220_op_pa_range(base + L2X0_CLEAN_INV_LINE_PA,
 				   start, end, flags);
 	l2c_wait_mask(base + L2X0_CLEAN_INV_LINE_PA, 1);
 	__l2c220_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_flush_all(void)
@@ -378,9 +422,9 @@ static void l2c220_sync(void)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c220_cache_sync(l2x0_base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void l2c220_enable(void __iomem *base, unsigned num_lock)
@@ -472,7 +516,7 @@ static void l2c310_inv_range_erratum(unsigned long start, unsigned long end)
 		unsigned long flags;
 
 		/* Erratum 588369 for both clean+invalidate operations */
-		raw_spin_lock_irqsave(&l2x0_lock, flags);
+		l2x0_spin_lock_irqsave(flags);
 		l2c_set_debug(base, 0x03);
 
 		if (start & (CACHE_LINE_SIZE - 1)) {
@@ -489,7 +533,7 @@ static void l2c310_inv_range_erratum(unsigned long start, unsigned long end)
 		}
 
 		l2c_set_debug(base, 0x00);
-		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		l2x0_spin_unlock_irqrestore(flags);
 	}
 
 	__l2c210_op_pa_range(base + L2X0_INV_LINE_PA, start, end);
@@ -502,7 +546,7 @@ static void l2c310_flush_range_erratum(unsigned long start, unsigned long end)
 	unsigned long flags;
 	void __iomem *base = l2x0_base;
 
-	raw_spin_lock_irqsave(lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	while (start < end) {
 		unsigned long blk_end = start + min(end - start, 4096UL);
 
@@ -515,11 +559,11 @@ static void l2c310_flush_range_erratum(unsigned long start, unsigned long end)
 		l2c_set_debug(base, 0x00);
 
 		if (blk_end < end) {
-			raw_spin_unlock_irqrestore(lock, flags);
-			raw_spin_lock_irqsave(lock, flags);
+			l2x0_spin_unlock_irqrestore(flags);
+			l2x0_spin_lock_irqsave(flags);
 		}
 	}
-	raw_spin_unlock_irqrestore(lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 	__l2c210_cache_sync(base);
 }
 
@@ -528,12 +572,12 @@ static void l2c310_flush_all_erratum(void)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	l2c_set_debug(base, 0x03);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
 	l2c_set_debug(base, 0x00);
 	__l2c210_cache_sync(base);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void __init l2c310_save(void __iomem *base)
@@ -774,6 +818,7 @@ static const struct l2c_init_data l2c310_init_fns __initconst = {
 		.disable = l2c310_disable,
 		.sync = l2c210_sync,
 		.resume = l2c310_resume,
+		.tz_mutex = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1100,6 +1145,7 @@ static const struct l2c_init_data of_l2c210_data __initconst = {
 		.disable     = l2c_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1120,6 +1166,7 @@ static const struct l2c_init_data of_l2c220_data __initconst = {
 		.disable     = l2c_disable,
 		.sync        = l2c220_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1308,6 +1355,7 @@ static const struct l2c_init_data of_l2c310_data __initconst = {
 		.disable     = l2c310_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1337,6 +1385,7 @@ static const struct l2c_init_data of_l2c310_coherent_data __initconst = {
 		.flush_all   = l2c210_flush_all,
 		.disable     = l2c310_disable,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1383,10 +1432,10 @@ static void aurora_pa_range(unsigned long start, unsigned long end,
 	while (start < end) {
 		range_end = aurora_range_end(start, end);
 
-		raw_spin_lock_irqsave(&l2x0_lock, flags);
+		l2x0_spin_lock_irqsave(flags);
 		writel_relaxed(start, base + AURORA_RANGE_BASE_ADDR_REG);
 		writel_relaxed(range_end - CACHE_LINE_SIZE, base + offset);
-		raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+		l2x0_spin_unlock_irqrestore(flags);
 
 		writel_relaxed(0, base + AURORA_SYNC_REG);
 		start = range_end;
@@ -1421,9 +1470,9 @@ static void aurora_flush_all(void)
 	unsigned long flags;
 
 	/* clean all ways */
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 
 	writel_relaxed(0, base + AURORA_SYNC_REG);
 }
@@ -1438,12 +1487,12 @@ static void aurora_disable(void)
 	void __iomem *base = l2x0_base;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&l2x0_lock, flags);
+	l2x0_spin_lock_irqsave(flags);
 	__l2c_op_way(base + L2X0_CLEAN_INV_WAY);
 	writel_relaxed(0, base + AURORA_SYNC_REG);
 	l2c_write_sec(0, base, L2X0_CTRL);
 	dsb(st);
-	raw_spin_unlock_irqrestore(&l2x0_lock, flags);
+	l2x0_spin_unlock_irqrestore(flags);
 }
 
 static void aurora_save(void __iomem *base)
@@ -1528,6 +1577,7 @@ static const struct l2c_init_data of_aurora_with_outer_data __initconst = {
 		.disable     = aurora_disable,
 		.sync	     = aurora_cache_sync,
 		.resume      = l2c_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
@@ -1700,6 +1750,7 @@ static const struct l2c_init_data of_bcm_l2x0_data __initconst = {
 		.disable     = l2c310_disable,
 		.sync        = l2c210_sync,
 		.resume      = l2c310_resume,
+		.tz_mutex    = l2x0_tz_mutex_cfg,
 	},
 };
 
diff --git a/drivers/clocksource/Kconfig b/drivers/clocksource/Kconfig
index f35a53ce8..d9ecff161 100644
--- a/drivers/clocksource/Kconfig
+++ b/drivers/clocksource/Kconfig
@@ -400,7 +400,7 @@ config ARM_TIMER_SP804
 
 config CLKSRC_ARM_GLOBAL_TIMER_SCHED_CLOCK
 	bool
-	depends on ARM_GLOBAL_TIMER
+	depends on ARM_GLOBAL_TIMER && !CPU_FREQ
 	default y
 	help
 	 Use ARM global timer clock source as sched_clock
diff --git a/drivers/i2c/busses/i2c-st.c b/drivers/i2c/busses/i2c-st.c
index 54e1fc8a4..c0a07e059 100644
--- a/drivers/i2c/busses/i2c-st.c
+++ b/drivers/i2c/busses/i2c-st.c
@@ -270,7 +270,7 @@ static void st_i2c_soft_reset(struct st_i2c_dev *i2c_dev)
 static void st_i2c_hw_config(struct st_i2c_dev *i2c_dev)
 {
 	unsigned long rate;
-	u32 val, ns_per_clk;
+	u32 val, ns_per_clk, prsc;
 	struct st_i2c_timings *t = &i2c_timings[i2c_dev->mode];
 
 	st_i2c_soft_reset(i2c_dev);
@@ -322,12 +322,13 @@ static void st_i2c_hw_config(struct st_i2c_dev *i2c_dev)
 
 	/* Prescalers set up */
 	val = rate / 10000000;
-	writel_relaxed(val, i2c_dev->base + SSC_PRSCALER);
-	writel_relaxed(val, i2c_dev->base + SSC_PRSCALER_DATAOUT);
+	prsc = (val > SSC_PRSC_VALUE) ? SSC_PRSC_VALUE : val;
+	writel_relaxed(prsc, i2c_dev->base + SSC_PRSCALER);
+	writel_relaxed(prsc, i2c_dev->base + SSC_PRSCALER_DATAOUT);
 
 	/* Noise suppression witdh */
-	val = i2c_dev->scl_min_width_us * rate / 100000000;
-	writel_relaxed(val, i2c_dev->base + SSC_NOISE_SUPP_WIDTH);
+	val = i2c_dev->scl_min_width_us * rate / 1000;
+	val /= (prsc * 10000);
 
 	/* Noise suppression max output data delay width */
 	val = i2c_dev->sda_min_width_us * rate / 100000000;
@@ -496,10 +497,30 @@ static void st_i2c_terminate_xfer(struct st_i2c_dev *i2c_dev)
 	st_i2c_clr_bits(i2c_dev->base + SSC_IEN, SSC_IEN_TEEN);
 	st_i2c_clr_bits(i2c_dev->base + SSC_I2C, SSC_I2C_STRTG);
 
+	/*
+	 * Some glitches happen on SDA line before the Repeated Start is
+	 * generated. These glitches are sometimes detected as Repeated
+	 * Start by the SSC state machine. It causes the Repeated Start
+	 * interrupt to fire too early, before the completion of the resquested
+	 * Repeated Start.
+	 * By clearing the Repeated Start status, we ensure the SSC state
+	 * machine is in the expected state before generating the Repeated
+	 * Start.
+	 *
+	 * The cause of the glitch is yet to be found, but even when found and
+	 * corrected, clearing the Repeated Start status will make the driver
+	 * more robust.
+	 *
+	 * For the sake of consistency, we also clear the Stop status even if
+	 * no problems encountered for now.
+	 */
+
 	if (c->stop) {
+		writel_relaxed(SSC_CLR_SSCSTOP, i2c_dev->base + SSC_CLR);
 		st_i2c_set_bits(i2c_dev->base + SSC_IEN, SSC_IEN_STOPEN);
 		st_i2c_set_bits(i2c_dev->base + SSC_I2C, SSC_I2C_STOPG);
 	} else {
+		writel_relaxed(SSC_CLR_REPSTRT, i2c_dev->base + SSC_CLR);
 		st_i2c_set_bits(i2c_dev->base + SSC_IEN, SSC_IEN_REPSTRTEN);
 		st_i2c_set_bits(i2c_dev->base + SSC_I2C, SSC_I2C_REPSTRTG);
 	}
-- 
2.17.1

